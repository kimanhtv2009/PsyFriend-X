import React, { useState, useEffect, useRef, FormEvent } from 'react';
import ReactDOM from 'react-dom/client';
// FIX: Removed LiveSession as it is not an exported member of @google/genai
import { GoogleGenAI, Chat, Modality, LiveServerMessage, Blob } from '@google/genai';
import { CHATBOT_PERSONALITY } from './personality';
import { KNOWLEDGE_BASE_STRUCTURED } from './knowledge';

interface Message {
  id: number;
  text: string;
  sender: 'user' | 'bot';
}

// --- Helper Functions for Audio Processing ---

function encode(bytes: Uint8Array): string {
  let binary = '';
  const len = bytes.byteLength;
  for (let i = 0; i < len; i++) {
    binary += String.fromCharCode(bytes[i]);
  }
  return btoa(binary);
}

function decode(base64: string): Uint8Array {
  const binaryString = atob(base64);
  const len = binaryString.length;
  const bytes = new Uint8Array(len);
  for (let i = 0; i < len; i++) {
    bytes[i] = binaryString.charCodeAt(i);
  }
  return bytes;
}

async function decodeAudioData(
  data: Uint8Array,
  ctx: AudioContext,
  sampleRate: number,
  numChannels: number,
): Promise<AudioBuffer> {
  const dataInt16 = new Int16Array(data.buffer);
  const frameCount = dataInt16.length / numChannels;
  const buffer = ctx.createBuffer(numChannels, frameCount, sampleRate);

  for (let channel = 0; channel < numChannels; channel++) {
    const channelData = buffer.getChannelData(channel);
    for (let i = 0; i < frameCount; i++) {
      channelData[i] = dataInt16[i * numChannels + channel] / 32768.0;
    }
  }
  return buffer;
}

function createBlob(data: Float32Array): Blob {
    const l = data.length;
    const int16 = new Int16Array(l);
    for (let i = 0; i < l; i++) {
        int16[i] = data[i] * 32768;
    }
    return {
        data: encode(new Uint8Array(int16.buffer)),
        mimeType: 'audio/pcm;rate=16000',
    };
}

const getRelevantKnowledge = (query: string): string => {
  const lowerCaseQuery = query.toLowerCase();
  const relevantChunks = KNOWLEDGE_BASE_STRUCTURED.filter(item =>
    item.keywords.some(keyword => lowerCaseQuery.includes(keyword))
  );
  if (relevantChunks.length === 0) {
    return '';
  }
  return relevantChunks.map(chunk => chunk.content).join('\n\n');
};


const App: React.FC = () => {
  const [messages, setMessages] = useState<Message[]>([
    {
      id: 1,
      text: "Xin ch√†o! M√¨nh l√† PsyFriend, ng∆∞·ªùi b·∫°n ƒë·ªìng h√†nh v·ªÅ t√¢m l√Ω h·ªçc ƒë∆∞·ªùng c·ªßa b·∫°n. üå±\nM√¨nh ·ªü ƒë√¢y ƒë·ªÉ l·∫Øng nghe v√† t·∫°o m·ªôt kh√¥ng gian an to√†n ƒë·ªÉ b·∫°n chia s·∫ª. B·∫°n ƒëang c·∫£m th·∫•y th·∫ø n√†o h√¥m nay?\nN·∫øu b·∫°n mu·ªën, ch√∫ng ta c√≥ th·ªÉ b·∫Øt ƒë·∫ßu v·ªõi m·ªôt b√†i kh·∫£o s√°t nh·ªè ƒë·ªÉ hi·ªÉu r√µ h∆°n v·ªÅ b·∫£n th√¢n.",
      sender: 'bot',
    },
  ]);
  const [inputValue, setInputValue] = useState<string>('');
  const [isLoading, setIsLoading] = useState<boolean>(false);
  const [isRecording, setIsRecording] = useState<boolean>(false);
  
  const chatRef = useRef<Chat | null>(null);
  const messagesEndRef = useRef<HTMLDivElement | null>(null);
  const aiRef = useRef<GoogleGenAI | null>(null);
  
  // Refs for voice chat
  // FIX: Replaced LiveSession with any since it is not an exported type.
  const sessionPromiseRef = useRef<Promise<any> | null>(null);
  const inputAudioContextRef = useRef<AudioContext | null>(null);
  const outputAudioContextRef = useRef<AudioContext | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const scriptProcessorRef = useRef<ScriptProcessorNode | null>(null);
  const sourcesRef = useRef<Set<AudioBufferSourceNode>>(new Set());
  let nextStartTime = 0;
  
  // Refs for transcription
  const currentInputTranscription = useRef<string>('');
  const currentOutputTranscription = useRef<string>('');

  useEffect(() => {
    try {
      const ai = new GoogleGenAI({ apiKey: process.env.API_KEY as string });
      aiRef.current = ai;
      chatRef.current = ai.chats.create({
        model: 'gemini-2.5_pro',
        config: {
          systemInstruction: CHATBOT_PERSONALITY,
        },
      });
    } catch (error) {
       console.error("L·ªói kh·ªüi t·∫°o Gemini:", error);
       setMessages(prev => [...prev, {id: Date.now(), text: "R·∫•t ti·∫øc, ƒë√£ c√≥ l·ªói x·∫£y ra khi k·∫øt n·ªëi v·ªõi AI. Vui l√≤ng ki·ªÉm tra API key v√† th·ª≠ l·∫°i.", sender: 'bot'}]);
    }

    return () => {
        // Cleanup on unmount
        stopVoiceSession();
    }
  }, []);

  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [messages, isLoading]);

  const handleSendMessage = async (e: FormEvent) => {
    e.preventDefault();
    if (!inputValue.trim() || isLoading || isRecording) return;

    const userMessage: Message = {
      id: Date.now(),
      text: inputValue,
      sender: 'user',
    };
    setMessages((prevMessages) => [...prevMessages, userMessage]);
    setInputValue('');
    setIsLoading(true);
    
    const retrievedKnowledge = getRelevantKnowledge(inputValue);
    
    let prompt: string;
    if (retrievedKnowledge) {
      prompt = `D·ª±a v√†o ng·ªØ li·ªáu ki·∫øn th·ª©c sau ƒë√¢y:\n\n---\n${retrievedKnowledge}\n---\n\nH√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: "${inputValue}"`;
    } else {
      prompt = `H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: "${inputValue}"`;
    }

    try {
      if (!chatRef.current) {
        throw new Error("Chat session ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o.");
      }
      
      const response = await chatRef.current.sendMessage({ message: prompt });
      
      const botMessage: Message = {
        id: Date.now() + 1,
        text: response.text,
        sender: 'bot',
      };
      setMessages((prevMessages) => [...prevMessages, botMessage]);
    } catch (error) {
      console.error('L·ªói khi g·ª≠i tin nh·∫Øn:', error);
      const errorMessage: Message = {
        id: Date.now() + 1,
        text: 'Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë. Vui l√≤ng th·ª≠ l·∫°i sau.',
        sender: 'bot',
      };
      setMessages((prevMessages) => [...prevMessages, errorMessage]);
    } finally {
      setIsLoading(false);
    }
  };

  const startVoiceSession = async () => {
    if (!aiRef.current || isRecording) return;
    setIsRecording(true);
    setIsLoading(true);

    try {
        streamRef.current = await navigator.mediaDevices.getUserMedia({ audio: true });

        // FIX: Cast window to any to access vendor-prefixed webkitAudioContext
        inputAudioContextRef.current = new ((window as any).AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 });
        // FIX: Cast window to any to access vendor-prefixed webkitAudioContext
        outputAudioContextRef.current = new ((window as any).AudioContext || (window as any).webkitAudioContext)({ sampleRate: 24000 });

        sessionPromiseRef.current = aiRef.current.live.connect({
            model: 'gemini-2.5-pro',
            callbacks: {
                onopen: () => {
                    if (!inputAudioContextRef.current || !streamRef.current || !sessionPromiseRef.current) return;
                    const source = inputAudioContextRef.current.createMediaStreamSource(streamRef.current);
                    scriptProcessorRef.current = inputAudioContextRef.current.createScriptProcessor(4096, 1, 1);
                    
                    scriptProcessorRef.current.onaudioprocess = (audioProcessingEvent) => {
                        const inputData = audioProcessingEvent.inputBuffer.getChannelData(0);
                        const pcmBlob = createBlob(inputData);
                        sessionPromiseRef.current?.then((session) => {
                            session.sendRealtimeInput({ media: pcmBlob });
                        });
                    };
                    source.connect(scriptProcessorRef.current);
                    scriptProcessorRef.current.connect(inputAudioContextRef.current.destination);
                },
                onmessage: async (message: LiveServerMessage) => {
                   if (message.serverContent?.inputTranscription) {
                        currentInputTranscription.current += message.serverContent.inputTranscription.text;
                   }
                   if (message.serverContent?.outputTranscription) {
                        currentOutputTranscription.current += message.serverContent.outputTranscription.text;
                   }

                   if (message.serverContent?.turnComplete) {
                        const userMessage: Message = { id: Date.now(), text: currentInputTranscription.current, sender: 'user' };
                        const botMessage: Message = { id: Date.now() + 1, text: currentOutputTranscription.current, sender: 'bot' };
                        
                        setMessages(prev => [...prev.filter(m => m.id !== -1), userMessage, botMessage]);
                        
                        currentInputTranscription.current = '';
                        currentOutputTranscription.current = '';
                   } else {
                        // Live update transcription
                        const liveUserMessage: Message = { id: -1, text: currentInputTranscription.current + "...", sender: 'user' };
                        setMessages(prev => [...prev.filter(m => m.id !== -1), liveUserMessage]);
                   }
                    
                   const base64Audio = message.serverContent?.modelTurn?.parts[0]?.inlineData?.data;
                   if (base64Audio && outputAudioContextRef.current) {
                        nextStartTime = Math.max(nextStartTime, outputAudioContextRef.current.currentTime);
                        const audioBuffer = await decodeAudioData(decode(base64Audio), outputAudioContextRef.current, 24000, 1);
                        const source = outputAudioContextRef.current.createBufferSource();
                        source.buffer = audioBuffer;
                        source.connect(outputAudioContextRef.current.destination);
                        source.addEventListener('ended', () => sourcesRef.current.delete(source));
                        source.start(nextStartTime);
                        nextStartTime += audioBuffer.duration;
                        sourcesRef.current.add(source);
                   }

                   if(message.serverContent?.interrupted) {
                       for (const source of sourcesRef.current.values()) {
                           source.stop();
                           sourcesRef.current.delete(source);
                       }
                       nextStartTime = 0;
                   }
                },
                onerror: (e: ErrorEvent) => {
                    console.error('L·ªói voice session:', e);
                    setMessages(prev => [...prev, { id: Date.now(), text: "L·ªói k·∫øt n·ªëi voice chat. Vui l√≤ng th·ª≠ l·∫°i.", sender: 'bot' }]);
                    stopVoiceSession();
                },
                onclose: (e: CloseEvent) => {
                    stopVoiceSession();
                },
            },
            config: {
                responseModalities: [Modality.AUDIO],
                inputAudioTranscription: {},
                outputAudioTranscription: {},
                systemInstruction: CHATBOT_PERSONALITY,
            },
        });
    } catch (error) {
        console.error('Kh√¥ng th·ªÉ b·∫Øt ƒë·∫ßu voice session:', error);
        setMessages(prev => [...prev, {id: Date.now(), text: "Kh√¥ng th·ªÉ truy c·∫≠p micro. Vui l√≤ng c·∫•p quy·ªÅn v√† th·ª≠ l·∫°i.", sender: 'bot'}]);
        stopVoiceSession();
    }
  };

  const stopVoiceSession = () => {
      setIsRecording(false);
      setIsLoading(false);

      sessionPromiseRef.current?.then(session => session.close());
      sessionPromiseRef.current = null;

      streamRef.current?.getTracks().forEach(track => track.stop());
      streamRef.current = null;

      scriptProcessorRef.current?.disconnect();
      scriptProcessorRef.current = null;

      inputAudioContextRef.current?.close();
      outputAudioContextRef.current?.close();
      inputAudioContextRef.current = null;
      outputAudioContextRef.current = null;
      
      currentInputTranscription.current = '';
      currentOutputTranscription.current = '';
      setMessages(prev => prev.filter(m => m.id !== -1));
  };
  
  const toggleVoiceChat = () => {
    if (isRecording) {
        stopVoiceSession();
    } else {
        startVoiceSession();
    }
  }

  return (
    <div className="chat-container">
      <header className="chat-header">
        <div className="header-logo-container">
          <img src="https://raw.githubusercontent.com/kimanhtv2009/PSYFRIEND/main/cropped_circle_image%20(2).png" alt="Logo" className="header-logo" />
        </div>
        <span>PsyFriend</span>
      </header>
      <main className="chat-messages">
        {messages.map((msg) => (
          <div key={msg.id} className={`message ${msg.sender}`}>
            {msg.text}
          </div>
        ))}
        {isLoading && !isRecording && (
          <div className="message bot loading">
            <span></span>
            <span></span>
            <span></span>
          </div>
        )}
        {isRecording && (
            <div className="message bot">ƒêang l·∫Øng nghe...</div>
        )}
        <div ref={messagesEndRef} />
      </main>
      <form className="chat-input-form" onSubmit={handleSendMessage}>
        <input
          type="text"
          className="chat-input"
          value={inputValue}
          onChange={(e) => setInputValue(e.target.value)}
          placeholder={isRecording ? "ƒêang l·∫Øng nghe..." : "Nh·∫≠p tin nh·∫Øn c·ªßa b·∫°n..."}
          aria-label="Chat input"
          disabled={isLoading || isRecording}
        />
        <button type="button" className={`mic-button ${isRecording ? 'recording' : ''}`} onClick={toggleVoiceChat} aria-label={isRecording ? 'Stop recording' : 'Start recording'}>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><path d="M12 1a3 3 0 0 0-3 3v8a3 3 0 0 0 6 0V4a3 3 0 0 0-3-3z"></path><path d="M19 10v2a7 7 0 0 1-14 0v-2"></path><line x1="12" y1="19" x2="12" y2="23"></line><line x1="8" y1="23" x2="16" y2="23"></line></svg>
        </button>
        <button type="submit" className="send-button" disabled={!inputValue.trim() || isLoading || isRecording} aria-label="Send message">
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><line x1="22" y1="2" x2="11" y2="13"></line><polygon points="22 2 15 22 11 13 2 9 22 2"></polygon></svg>
        </button>
      </form>
    </div>
  );
};

const root = ReactDOM.createRoot(document.getElementById('root') as HTMLElement);
root.render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
);
